{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58b9b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "157cfb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import torch\n",
    "import os\n",
    "import PIL\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.datasets.utils import download_file_from_google_drive, check_integrity, verify_str_arg\n",
    "\n",
    "\n",
    "class CelebA(VisionDataset):\n",
    "    \"\"\"`Large-scale CelebFaces Attributes (CelebA) Dataset <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory where images are downloaded to.\n",
    "        split (string): One of {'train', 'valid', 'test', 'all'}.\n",
    "            Accordingly dataset is selected.\n",
    "        target_type (string or list, optional): Type of target to use, ``attr``, ``identity``, ``bbox``,\n",
    "            or ``landmarks``. Can also be a list to output a tuple with all specified target types.\n",
    "            The targets represent:\n",
    "                ``attr`` (np.array shape=(40,) dtype=int): binary (0, 1) labels for attributes\n",
    "                ``identity`` (int): label for each person (data points with the same identity are the same person)\n",
    "                ``bbox`` (np.array shape=(4,) dtype=int): bounding box (x, y, width, height)\n",
    "                ``landmarks`` (np.array shape=(10,) dtype=int): landmark points (lefteye_x, lefteye_y, righteye_x,\n",
    "                    righteye_y, nose_x, nose_y, leftmouth_x, leftmouth_y, rightmouth_x, rightmouth_y)\n",
    "            Defaults to ``attr``. If empty, ``None`` will be returned as target.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "\n",
    "    base_folder = \"celeba\"\n",
    "    # There currently does not appear to be a easy way to extract 7z in python (without introducing additional\n",
    "    # dependencies). The \"in-the-wild\" (not aligned+cropped) images are only in 7z, so they are not available\n",
    "    # right now.\n",
    "    file_list = [\n",
    "        # File ID                         MD5 Hash                            Filename\n",
    "        (\"0B7EVK8r0v71pTUZsaXdaSnZBZzg\", \"00d2c5bc6d35e252742224ab0c1e8fcb\", \"img_align_celeba.zip\"),\n",
    "        # (\"0B7EVK8r0v71pbWNEUjJKdDQ3dGc\", \"b6cd7e93bc7a96c2dc33f819aa3ac651\", \"img_align_celeba_png.7z\"),\n",
    "        # (\"0B7EVK8r0v71peklHb0pGdDl6R28\", \"b6cd7e93bc7a96c2dc33f819aa3ac651\", \"img_celeba.7z\"),\n",
    "        (\"0B7EVK8r0v71pblRyaVFSWGxPY0U\", \"75e246fa4810816ffd6ee81facbd244c\", \"list_attr_celeba.txt\"),\n",
    "        (\"1_ee_0u7vcNLOfNLegJRHmolfH5ICW-XS\", \"32bd1bd63d3c78cd57e08160ec5ed1e2\", \"identity_CelebA.txt\"),\n",
    "        (\"0B7EVK8r0v71pbThiMVRxWXZ4dU0\", \"00566efa6fedff7a56946cd1c10f1c16\", \"list_bbox_celeba.txt\"),\n",
    "        (\"0B7EVK8r0v71pd0FJY3Blby1HUTQ\", \"cc24ecafdb5b50baae59b03474781f8c\", \"list_landmarks_align_celeba.txt\"),\n",
    "        # (\"0B7EVK8r0v71pTzJIdlJWdHczRlU\", \"063ee6ddb681f96bc9ca28c6febb9d1a\", \"list_landmarks_celeba.txt\"),\n",
    "        (\"0B7EVK8r0v71pY0NSMzRuSXJEVkk\", \"d32c9cbf5e040fd4025c592c306e6668\", \"list_eval_partition.txt\"),\n",
    "    ]\n",
    "\n",
    "    def __init__(self, root, split=\"train\", target_type=\"attr\", transform=None,\n",
    "                 target_transform=None, download=False):\n",
    "        import pandas\n",
    "        super(CelebA, self).__init__(root, transform=transform,\n",
    "                                     target_transform=target_transform)\n",
    "        self.split = split\n",
    "        if isinstance(target_type, list):\n",
    "            self.target_type = target_type\n",
    "        else:\n",
    "            self.target_type = [target_type]\n",
    "\n",
    "        if not self.target_type and self.target_transform is not None:\n",
    "            raise RuntimeError('target_transform is specified but target_type is empty')\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "#         print(f\"{self._check_integrity()=}\")\n",
    "#         if not self._check_integrity():\n",
    "#             raise RuntimeError('Dataset not found or corrupted.' +\n",
    "#                                ' You can use download=True to download it')\n",
    "\n",
    "        split_map = {\n",
    "            \"train\": 0,\n",
    "            \"valid\": 1,\n",
    "            \"test\": 2,\n",
    "            \"all\": None,\n",
    "        }\n",
    "        split = split_map[verify_str_arg(split.lower(), \"split\",\n",
    "                                         (\"train\", \"valid\", \"test\", \"all\"))]\n",
    "\n",
    "        fn = partial(os.path.join, self.root, self.base_folder)\n",
    "        splits = pandas.read_csv(fn(\"list_eval_partition.txt\"), delim_whitespace=True, header=None, index_col=0)\n",
    "        identity = pandas.read_csv(fn(\"identity_CelebA.txt\"), delim_whitespace=True, header=None, index_col=0)\n",
    "        bbox = pandas.read_csv(fn(\"list_bbox_celeba.txt\"), delim_whitespace=True, header=1, index_col=0)\n",
    "        landmarks_align = pandas.read_csv(fn(\"list_landmarks_align_celeba.txt\"), delim_whitespace=True, header=1)\n",
    "        attr = pandas.read_csv(fn(\"list_attr_celeba.txt\"), delim_whitespace=True, header=1)\n",
    "\n",
    "        mask = slice(None) if split is None else (splits[1] == split)\n",
    "\n",
    "        self.filename = splits[mask].index.values\n",
    "        self.identity = torch.as_tensor(identity[mask].values)\n",
    "        self.bbox = torch.as_tensor(bbox[mask].values)\n",
    "        self.landmarks_align = torch.as_tensor(landmarks_align[mask].values)\n",
    "        self.attr = torch.as_tensor(attr[mask].values)\n",
    "        self.attr = (self.attr + 1) // 2  # map from {-1, 1} to {0, 1}\n",
    "        self.attr_names = list(attr.columns)\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        for (_, md5, filename) in self.file_list:\n",
    "            print(f\"{filename=}\\t{md5=}\")\n",
    "            fpath = os.path.join(self.root, self.base_folder, filename)\n",
    "            _, ext = os.path.splitext(filename)\n",
    "            # Allow original archive to be deleted (zip and 7z)\n",
    "            # Only need the extracted images\n",
    "            print(check_integrity(fpath, md5))\n",
    "            if ext not in [\".zip\", \".7z\"] and not check_integrity(fpath, md5):\n",
    "                return False\n",
    "        # Should check a hash of the images\n",
    "        return os.path.isdir(os.path.join(self.root, self.base_folder, \"img_align_celeba\"))\n",
    "\n",
    "    def download(self):\n",
    "        import zipfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        for (file_id, md5, filename) in self.file_list:\n",
    "            download_file_from_google_drive(file_id, os.path.join(self.root, self.base_folder), filename, md5)\n",
    "\n",
    "        with zipfile.ZipFile(os.path.join(self.root, self.base_folder, \"img_align_celeba.zip\"), \"r\") as f:\n",
    "            f.extractall(os.path.join(self.root, self.base_folder))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = PIL.Image.open(os.path.join(self.root, self.base_folder, \"img_align_celeba\", self.filename[index]))\n",
    "\n",
    "        target = []\n",
    "        for t in self.target_type:\n",
    "            if t == \"attr\":\n",
    "                target.append(self.attr[index, :])\n",
    "            elif t == \"identity\":\n",
    "                target.append(self.identity[index, 0])\n",
    "            elif t == \"bbox\":\n",
    "                target.append(self.bbox[index, :])\n",
    "            elif t == \"landmarks\":\n",
    "                target.append(self.landmarks_align[index, :])\n",
    "            else:\n",
    "                # TODO: refactor with utils.verify_str_arg\n",
    "                raise ValueError(\"Target type \\\"{}\\\" is not recognized.\".format(t))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "\n",
    "        if target:\n",
    "            target = tuple(target) if len(target) > 1 else target[0]\n",
    "\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "        else:\n",
    "            target = None\n",
    "\n",
    "        return X, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.attr)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        lines = [\"Target type: {target_type}\", \"Split: {split}\"]\n",
    "        return '\\n'.join(lines).format(**self.__dict__)\n",
    "    \n",
    "    \n",
    "#this code is to settle the 'file not found error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c04982c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform =transforms.Compose([transforms.ToTensor()]) #didn't add transforms.Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2aaf771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98d4e77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/45/f7c2jlxs4296lyr5sxsr5wfc0000gn/T/ipykernel_15755/3867873349.py:96: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self.attr = (self.attr + 1) // 2  # map from {-1, 1} to {0, 1}\n"
     ]
    }
   ],
   "source": [
    "trainset = CelebA('./',split='train', target_type='identity',download=False, transform=transform, target_transform = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1291bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 218, 178])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8363288",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0662cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = [0,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "348c7804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for i in iterator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e21c1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_iterator = iter(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f4dc868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(my_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d132dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28e6b7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_loader = iter(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "590f7fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9997a57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 218, 178])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape #image shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf647e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8468, 2300, 3249, 1373, 3357, 7099, 1245,  289])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6ecef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65ee55b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/45/f7c2jlxs4296lyr5sxsr5wfc0000gn/T/ipykernel_15755/3867873349.py:96: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self.attr = (self.attr + 1) // 2  # map from {-1, 1} to {0, 1}\n"
     ]
    }
   ],
   "source": [
    "testset = CelebA('./',split='test', target_type='identity',download=False, transform=transform, target_transform = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d5d552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f36fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,5)    #3 = colourful image channel\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.fc1 = nn.Linear(16*51*41,120) \n",
    "        #have to follow input image, x.shape before flattern: 16*51*41= 33456\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10178)\n",
    "        # have 10178 class\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "    \n",
    "        x = torch.flatten(x,1)\n",
    "       \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "#define the network\n",
    "\n",
    "# print(f'before flatten x.shape = {x.shape}')\n",
    "# print(f'x.shape = {x.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "638f7b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1353,  0.0044, -0.0500,  ..., -0.0588, -0.0524,  0.0212],\n",
       "        [-0.1337,  0.0153, -0.0397,  ..., -0.0573, -0.0465,  0.0162],\n",
       "        [-0.1258,  0.0197, -0.0437,  ..., -0.0627, -0.0490,  0.0158],\n",
       "        ...,\n",
       "        [-0.1307,  0.0159, -0.0423,  ..., -0.0576, -0.0552,  0.0178],\n",
       "        [-0.1171,  0.0059, -0.0509,  ..., -0.0594, -0.0565,  0.0129],\n",
       "        [-0.1250, -0.0009, -0.0513,  ..., -0.0614, -0.0600,  0.0124]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(batch[0])\n",
    "\n",
    "#before flatten x.shape = torch.Size([8, 16, 51, 41]) --> each batch 8 image\n",
    "#x.shape = torch.Size([8, 33456]) --> after flatting become 33456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abe4fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#define loss function and optimizer\n",
    "\n",
    "#normally use cross-entropyloss for classification problem\n",
    "#nn.MSELoss for regression problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f8cb0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name, i in net.named_parameters():\n",
    "    #print(f\"{name}\")\n",
    "    \n",
    "#list out the parameters that can be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b6b865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 20347/20347 [33:50<00:00\n",
      "100%|█| 20347/20347 [34:25<00:00\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    Loss = 0\n",
    "    for inputs, labels in tqdm.tqdm(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "#inputs is the image\n",
    "#outputs is the labels \n",
    "\n",
    "#Train the network in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "731d8278",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './CelebA_net.pt'\n",
    "torch.save(net.state_dict(), PATH)\n",
    "#save the trained model as PATH\n",
    "\n",
    "#for future use, open empty jupyter\n",
    "#1.define the network\n",
    "#2.net = Net()\n",
    "#net.load_state_dict(torch.load(PATH)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15c7266f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 218, 178])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dataiter = iter(testloader)\n",
    "inputs, labels = dataiter.next()\n",
    "print(f'{(inputs.shape)}')\n",
    "\n",
    "#iterate testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea3aa7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "#load back saved model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e32f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(inputs)\n",
    "#subs testing inputs into saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a52e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "#print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              #for j in range(8)))\n",
    "\n",
    "#test the network on the test data\n",
    "#NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "858e0099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 2496/2496 [01:39<00:00, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "#initialize number of testset\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm.tqdm(testloader):\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        #max function will give 2 outcome\n",
    "        #x_random = torch.randn(8,10)\n",
    "        #torch.max(x_random,1) \n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy: %d %%' % (100 * correct / total))\n",
    "\n",
    "#calculate the accuracy of the network on the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31850b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8.10",
   "language": "python",
   "name": "python3.8.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
