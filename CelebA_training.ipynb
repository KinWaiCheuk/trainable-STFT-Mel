{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58b9b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e32517e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import torch\n",
    "import os\n",
    "import PIL\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.datasets.utils import download_file_from_google_drive, check_integrity, verify_str_arg\n",
    "\n",
    "\n",
    "class CelebA(VisionDataset):\n",
    "    \"\"\"`Large-scale CelebFaces Attributes (CelebA) Dataset <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory where images are downloaded to.\n",
    "        split (string): One of {'train', 'valid', 'test', 'all'}.\n",
    "            Accordingly dataset is selected.\n",
    "        target_type (string or list, optional): Type of target to use, ``attr``, ``identity``, ``bbox``,\n",
    "            or ``landmarks``. Can also be a list to output a tuple with all specified target types.\n",
    "            The targets represent:\n",
    "                ``attr`` (np.array shape=(40,) dtype=int): binary (0, 1) labels for attributes\n",
    "                ``identity`` (int): label for each person (data points with the same identity are the same person)\n",
    "                ``bbox`` (np.array shape=(4,) dtype=int): bounding box (x, y, width, height)\n",
    "                ``landmarks`` (np.array shape=(10,) dtype=int): landmark points (lefteye_x, lefteye_y, righteye_x,\n",
    "                    righteye_y, nose_x, nose_y, leftmouth_x, leftmouth_y, rightmouth_x, rightmouth_y)\n",
    "            Defaults to ``attr``. If empty, ``None`` will be returned as target.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "\n",
    "    base_folder = \"celeba\"\n",
    "    # There currently does not appear to be a easy way to extract 7z in python (without introducing additional\n",
    "    # dependencies). The \"in-the-wild\" (not aligned+cropped) images are only in 7z, so they are not available\n",
    "    # right now.\n",
    "    file_list = [\n",
    "        # File ID                         MD5 Hash                            Filename\n",
    "        (\"0B7EVK8r0v71pTUZsaXdaSnZBZzg\", \"00d2c5bc6d35e252742224ab0c1e8fcb\", \"img_align_celeba.zip\"),\n",
    "        # (\"0B7EVK8r0v71pbWNEUjJKdDQ3dGc\", \"b6cd7e93bc7a96c2dc33f819aa3ac651\", \"img_align_celeba_png.7z\"),\n",
    "        # (\"0B7EVK8r0v71peklHb0pGdDl6R28\", \"b6cd7e93bc7a96c2dc33f819aa3ac651\", \"img_celeba.7z\"),\n",
    "        (\"0B7EVK8r0v71pblRyaVFSWGxPY0U\", \"75e246fa4810816ffd6ee81facbd244c\", \"list_attr_celeba.txt\"),\n",
    "        (\"1_ee_0u7vcNLOfNLegJRHmolfH5ICW-XS\", \"32bd1bd63d3c78cd57e08160ec5ed1e2\", \"identity_CelebA.txt\"),\n",
    "        (\"0B7EVK8r0v71pbThiMVRxWXZ4dU0\", \"00566efa6fedff7a56946cd1c10f1c16\", \"list_bbox_celeba.txt\"),\n",
    "        (\"0B7EVK8r0v71pd0FJY3Blby1HUTQ\", \"cc24ecafdb5b50baae59b03474781f8c\", \"list_landmarks_align_celeba.txt\"),\n",
    "        # (\"0B7EVK8r0v71pTzJIdlJWdHczRlU\", \"063ee6ddb681f96bc9ca28c6febb9d1a\", \"list_landmarks_celeba.txt\"),\n",
    "        (\"0B7EVK8r0v71pY0NSMzRuSXJEVkk\", \"d32c9cbf5e040fd4025c592c306e6668\", \"list_eval_partition.txt\"),\n",
    "    ]\n",
    "\n",
    "    def __init__(self, root, split=\"train\", target_type=\"attr\", transform=None,\n",
    "                 target_transform=None, download=False):\n",
    "        import pandas\n",
    "        super(CelebA, self).__init__(root, transform=transform,\n",
    "                                     target_transform=target_transform)\n",
    "        self.split = split\n",
    "        if isinstance(target_type, list):\n",
    "            self.target_type = target_type\n",
    "        else:\n",
    "            self.target_type = [target_type]\n",
    "\n",
    "        if not self.target_type and self.target_transform is not None:\n",
    "            raise RuntimeError('target_transform is specified but target_type is empty')\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "#         print(f\"{self._check_integrity()=}\")\n",
    "#         if not self._check_integrity():\n",
    "#             raise RuntimeError('Dataset not found or corrupted.' +\n",
    "#                                ' You can use download=True to download it')\n",
    "\n",
    "        split_map = {\n",
    "            \"train\": 0,\n",
    "            \"valid\": 1,\n",
    "            \"test\": 2,\n",
    "            \"all\": None,\n",
    "        }\n",
    "        split = split_map[verify_str_arg(split.lower(), \"split\",\n",
    "                                         (\"train\", \"valid\", \"test\", \"all\"))]\n",
    "\n",
    "        fn = partial(os.path.join, self.root, self.base_folder)\n",
    "        splits = pandas.read_csv(fn(\"list_eval_partition.txt\"), delim_whitespace=True, header=None, index_col=0)\n",
    "        identity = pandas.read_csv(fn(\"identity_CelebA.txt\"), delim_whitespace=True, header=None, index_col=0)\n",
    "        bbox = pandas.read_csv(fn(\"list_bbox_celeba.txt\"), delim_whitespace=True, header=1, index_col=0)\n",
    "        landmarks_align = pandas.read_csv(fn(\"list_landmarks_align_celeba.txt\"), delim_whitespace=True, header=1)\n",
    "        attr = pandas.read_csv(fn(\"list_attr_celeba.txt\"), delim_whitespace=True, header=1)\n",
    "\n",
    "        mask = slice(None) if split is None else (splits[1] == split)\n",
    "\n",
    "        self.filename = splits[mask].index.values\n",
    "        self.identity = torch.as_tensor(identity[mask].values)\n",
    "        self.bbox = torch.as_tensor(bbox[mask].values)\n",
    "        self.landmarks_align = torch.as_tensor(landmarks_align[mask].values)\n",
    "        self.attr = torch.as_tensor(attr[mask].values)\n",
    "        self.attr = (self.attr + 1) // 2  # map from {-1, 1} to {0, 1}\n",
    "        self.attr_names = list(attr.columns)\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        for (_, md5, filename) in self.file_list:\n",
    "            print(f\"{filename=}\\t{md5=}\")\n",
    "            fpath = os.path.join(self.root, self.base_folder, filename)\n",
    "            _, ext = os.path.splitext(filename)\n",
    "            # Allow original archive to be deleted (zip and 7z)\n",
    "            # Only need the extracted images\n",
    "            print(check_integrity(fpath, md5))\n",
    "            if ext not in [\".zip\", \".7z\"] and not check_integrity(fpath, md5):\n",
    "                return False\n",
    "        # Should check a hash of the images\n",
    "        return os.path.isdir(os.path.join(self.root, self.base_folder, \"img_align_celeba\"))\n",
    "\n",
    "    def download(self):\n",
    "        import zipfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        for (file_id, md5, filename) in self.file_list:\n",
    "            download_file_from_google_drive(file_id, os.path.join(self.root, self.base_folder), filename, md5)\n",
    "\n",
    "        with zipfile.ZipFile(os.path.join(self.root, self.base_folder, \"img_align_celeba.zip\"), \"r\") as f:\n",
    "            f.extractall(os.path.join(self.root, self.base_folder))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = PIL.Image.open(os.path.join(self.root, self.base_folder, \"img_align_celeba\", self.filename[index]))\n",
    "\n",
    "        target = []\n",
    "        for t in self.target_type:\n",
    "            if t == \"attr\":\n",
    "                target.append(self.attr[index, :])\n",
    "            elif t == \"identity\":\n",
    "                target.append(self.identity[index, 0])\n",
    "            elif t == \"bbox\":\n",
    "                target.append(self.bbox[index, :])\n",
    "            elif t == \"landmarks\":\n",
    "                target.append(self.landmarks_align[index, :])\n",
    "            else:\n",
    "                # TODO: refactor with utils.verify_str_arg\n",
    "                raise ValueError(\"Target type \\\"{}\\\" is not recognized.\".format(t))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "\n",
    "        if target:\n",
    "            target = tuple(target) if len(target) > 1 else target[0]\n",
    "\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "        else:\n",
    "            target = None\n",
    "\n",
    "        return X, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.attr)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        lines = [\"Target type: {target_type}\", \"Split: {split}\"]\n",
    "        return '\\n'.join(lines).format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c04982c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform =transforms.Compose([transforms.ToTensor()]) #didn't add transforms.Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aaf771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98d4e77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/45/f7c2jlxs4296lyr5sxsr5wfc0000gn/T/ipykernel_15331/616275015.py:96: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self.attr = (self.attr + 1) // 2  # map from {-1, 1} to {0, 1}\n"
     ]
    }
   ],
   "source": [
    "trainset = CelebA('./',split='train', target_type='identity',download=False, transform=transform, target_transform = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1291bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.9922, 0.9922, 0.9922,  ..., 0.9686, 0.9961, 0.9961],\n",
       "          [0.9922, 0.9922, 0.9922,  ..., 0.9765, 0.9961, 0.9961],\n",
       "          [0.9922, 0.9922, 0.9922,  ..., 0.9804, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.5490, 0.4549, 0.5725,  ..., 0.4784, 0.4784, 0.4784],\n",
       "          [0.5098, 0.5412, 0.6510,  ..., 0.4627, 0.4627, 0.4627],\n",
       "          [0.6588, 0.8000, 0.9608,  ..., 0.4627, 0.4706, 0.4706]],\n",
       " \n",
       "         [[0.9059, 0.9059, 0.9059,  ..., 0.8863, 0.9333, 0.9333],\n",
       "          [0.9059, 0.9059, 0.9059,  ..., 0.8941, 0.9333, 0.9333],\n",
       "          [0.9059, 0.9059, 0.9059,  ..., 0.9059, 0.9373, 0.9373],\n",
       "          ...,\n",
       "          [0.2902, 0.1882, 0.3059,  ..., 0.2157, 0.2196, 0.2196],\n",
       "          [0.2431, 0.2745, 0.3843,  ..., 0.1922, 0.2000, 0.2000],\n",
       "          [0.3922, 0.5333, 0.6941,  ..., 0.1922, 0.1961, 0.1961]],\n",
       " \n",
       "         [[0.7608, 0.7608, 0.7608,  ..., 0.8824, 0.8706, 0.8706],\n",
       "          [0.7608, 0.7608, 0.7608,  ..., 0.8824, 0.8706, 0.8706],\n",
       "          [0.7608, 0.7608, 0.7608,  ..., 0.8902, 0.8745, 0.8745],\n",
       "          ...,\n",
       "          [0.1020, 0.0039, 0.1294,  ..., 0.1098, 0.1176, 0.1176],\n",
       "          [0.0588, 0.0902, 0.2078,  ..., 0.0784, 0.0941, 0.0941],\n",
       "          [0.2078, 0.3490, 0.5176,  ..., 0.0784, 0.0941, 0.0941]]]),\n",
       " tensor(2880))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8363288",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93474da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset = torchvision.datasets.CelebA('./',train=False,download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a71283e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/45/f7c2jlxs4296lyr5sxsr5wfc0000gn/T/ipykernel_15331/616275015.py:96: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self.attr = (self.attr + 1) // 2  # map from {-1, 1} to {0, 1}\n"
     ]
    }
   ],
   "source": [
    "testset = CelebA('./',split='test', target_type='identity',download=False, transform=transform, target_transform = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d5d552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f36fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,5)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.fc1 = nn.Linear(16*4*4,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "#define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abe4fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#define loss function and optimizer\n",
    "\n",
    "#normally use cross-entropyloss for classification problem\n",
    "#nn.MSELoss for regression problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f8cb0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name, i in net.named_parameters():\n",
    "    #print(f\"{name}\")\n",
    "    \n",
    "#list out the parameters that can be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f2949737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 1, 2, 4, 7, 0, 6, 7])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b6b865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    Loss = 0\n",
    "    for inputs, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "#inputs is the image\n",
    "#outputs is the labels \n",
    "\n",
    "#Train the network in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "731d8278",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './Mnist_net.pt'\n",
    "torch.save(net.state_dict(), PATH)\n",
    "#save the trained model as PATH\n",
    "\n",
    "#for future use, open empty jupyter\n",
    "#1.define the network\n",
    "#2.net = Net()\n",
    "#net.load_state_dict(torch.load(PATH)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15c7266f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dataiter = iter(testloader)\n",
    "inputs, labels = dataiter.next()\n",
    "print(f'{(inputs.shape)}')\n",
    "\n",
    "#iterate testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad3955dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA54AAACACAYAAABju7aRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAizklEQVR4nO3deXyUxf3A8e8kJCEk3JeRG0IUUUFEQVsFi1qKRVutB1VLLRVPCrWKVPurZ71rURERxdsi3opFEbylCnhwIzcKyhGQGwlk8/z+IM48s80my26e3SeTz/v1yovvZGZ355VvdrMP+50Z5XmeAAAAAAAQlIx0TwAAAAAA4DYuPAEAAAAAgeLCEwAAAAAQKC48AQAAAACB4sITAAAAABAoLjwBAAAAAIFK6sJTKdVfKbVEKbVcKTWquiaF1CKPbiCPbiCPbiCPbiCPbiCPbiCPNZ9K9BxPpVSmiCwVkVNEZK2IzBaRQZ7nLaq+6SFo5NEN5NEN5NEN5NEN5NEN5NEN5NENdZK47bEistzzvJUiIkqp50TkDBGJ+QuQrXK8upKXxEMiUTtkyybP85pX0EUeaxDy6Aby6Aby6Aby6Aby6Aby6IZYeUzmwrOViKzxtdeKSK/KblBX8qSX6pfEQyJR070Xv47RRR5rEPLoBvLoBvLoBvLoBvLoBvLohlh5TObCMy5KqaEiMlREpK7UC/rhEBDy6Aby6Aby6Aby6Aby6Aby6AbyGG7JbC70rYi08bVbl3/P4nneeM/zenqe1zNLcpJ4OASEPLqBPLqBPLqBPLqBPLqBPLqBPDogmQvP2SLSWSnVQSmVLSLnicjr1TMtpBB5dAN5dAN5dAN5dAN5dAN5dAN5dEDCpbae55Uqpa4Ukakikikij3met7DaZoaUII9uII9uII9uII9uII9uII9uII9uSGqNp+d5U0RkSjXNBWlCHt1AHt1AHt1AHt1AHt1AHt1AHmu+ZEptAQAAAACoEheeAAAAAIBAceEJAAAAAAgUF54AAAAAgEBx4QkAAAAACBQXngAAAACAQHHhCQAAAAAIVFLneAIAAKBmyqhbV8dL7+hu9ZVll+lYecrqy9pqPrfocPMXOvZKSqp5hgBcwieeAAAAAIBAceEJAAAAAAgUpbYAapyM+vWt9uYzD9fxtH/ca/V9WZKn49suvFDH6r9zA5pd7bRp6HE6Lh2w1erbsbWejo8vWhHzPv67tJOOc1blWH31V3s6bvzkJ4lOE6h1ol8vl97SVcd9ey/Q8RttxlrjMpX5bCLilUksvVZfoeOmj/DcdMFZizda7cdWHa/jhgOWp3o64aVMCXpGbq7VtfHCbjreduIeHS8/6XFrXGXPLb+uMwbrOPdD+zndYuxM0yiLxHV/6cInngAAAACAQHHhCQAAAAAIVK0vtd14+fFW+4eWFY8rOP5bq/3NggIdN1xq7/bW/CFKTYDqpnJM6eWaJ9tafXN7Pehr2SWaJ9Qt1fHXw025Zvv/Vu/8XJGRl2e1VbtWOl7yxyZW3zX9J+t4SMMxOt4U+cEa99Eecx/XTR4U87F9VUuS2W2b1Tft4kd1vOAGk+NLH7rSGtd63Hwdl+3YEfOxANf4S2q9QvMauemWfda4pUfZJbU/enZHi5j3na3s8r2z8zfreHMvc/9NH4lvrgif764274eHNBhj9S07yCyRmJeyGYVfnQ7tdPzqRy9F9X5U4W32eRV+W0RENkT97fSb/5MnTOMndt8RLczfwXY3zjIdISy75RNPAAAAAECguPAEAAAAAASKC08AAAAAQKBq5RrPI78wC4muaX631dcwI7vC22REXaOXHWa2P95Wttfq+0n7q3Xc8VrWe6ZE7yOt5orfmOMbHv61WXTSLzd2vfs7P2Ra7bvPOU/H3ucLk50hEpFhcrJ2YqGO5/Z6Oh2zcVZmo4Y63jKxmdX3/pHPxrzdtevNESqHTh2o47ZT7edZ9luzddxJPk1ojn2G/UXHM0fdp+PPRtxnjeuRNVzHrW9zbyHv82vtvyn5yqx39R9/Ec2/Zf99WwqtvrFvnZrUnOoW24/b6k73fu41QfF55lipmTc+GHPc/L1mTebZn1yi406DF1njvH3mvU30kSz3+B4rq6P5/prr7X0z2vyD34VQ8y2sP+SMpTqOfi159e3eOu4ovK+Nxz7P/B2csK2zjsc+f1rM23R4vjhm36pzmuu47LCdVt+CIWZN7sBXfqdj78vwvXflE08AAAAAQKC48AQAAAAABMrZUtvMrodY7eUXmmMAnm5+j46jS2u/j5To+Mb1p5j7U/b+x13zzPEqf2y40uqb8Vtz//1Xm7Jbjlk5cJG+Paz29mvM8QjTuj2l4yyZZY3LURX/akcq2ca6b90yq71v0os6vubhITo++G5Kh4ISXc619qk2Op57LOW1Qfnqpi46XnzkA1Zfse818aR/X2P1Fd4818S7EyuhjVer19eYxqhAHyrULl79S6tdv05JhePGtfkg5n0Ma7zMbg9aFmNkfMrEfu3cfeW+GCNFlpVm6fiCiX/ScZvp9pKVOu9+ntScaoOMbl2s9tjr7ve1zDKFz/fape8XTBqh446jzPuS6D+PP5xxrI6VnWLJW2/us/nM7WZOW+wSwFJBmK3/k1kuMaWjKde8c7P9Hrpo/Dodk1OfMvPEiD4K5YRpI3RcNOQzHbeV2O8hKzv8pO1N5nX6+z8cZ3f+tPJphgmfeAIAAAAAAsWFJwAAAAAgUFx4AgAAAAAC5dQaT5Vl1msuuz7X6lvUx9Sul4kZd806e+vvWf86WscNn429Zml1kVn7sGxiS6vvzoPMmomBl36o49kvt7LGRTZsjHn/2G/D8D1We073Sb5WjgTplFxTrz/uMvP7c8v7g61x3uz5gc7Ddf7n7Zon21p9sdZ1Ltxnrwf71RvmCI33T/+n1dcqs56gcl3uWK3jHhuGW30tZ5k1hB2n2+vUo5Z9pUz08VZ+ke5mHbj/mBgRkcjWbYHNKVV2nLzLbscYd0buyVZ7yf+ZNVuF3dZafW8c+lpSc4rOR35G7Nfmo3zbKiwcbF5XV51vv9ZfdpFZ/8l6T6Psp911XDzSXlN2dLZZ1zm7xKzYHHnVFda4Dq9WvN9E9JrRDx8ar2P/cTzRxm1rp+N5O1tbfTNeMu+x2k5YYvVFNm2OeZ8IRmbLFlb7skvMc39Lmfl9evPavta4nJWzBf+rdPU3Oj7/iqusvkPXmPXO1fG3sk7H9jouOX1rNdxjelT5iadS6jGl1Eal1ALf95oopaYppZaV/9s42GmiGrQnj04gj24gj24gj24gj24gj24gjw6Lp9T2CRHpH/W9USLyjud5nUXkHanVewzWGJuEPLqAPLqBPLqBPLqBPLqBPLqBPDqsylJbz/M+VEq1j/r2GSLStzx+UkTeF5Frq3NiifhmZE8dL+hzX1Svuca+pdgc0bHiHLv8teHK+I4EiCxdoeM5fzvG7nzUlLFc12yOjm96K9MaNudUU6IbKS6O63GTsFNEvo/6Xjjz+MIROp537BNRvSqlc/lRb1/l2IZe9pEfLVJbgVJj8hivnWccpeO5vR6KPc4zJZ/nj/mL1df5HrM9+a5fRv1/mv20C4tQ5bF03Xodt75tfSUjwyH6+A6/0T2e1/Ftfe2y+NxXZ0UPT1bK81i2Z0/Vg0REosYVXmX+tmXUs8vPf9V4oI6XDjdlk2UFcT5WlBuOmazjQfU3xHWbDnXqWu28G8yRZSXvJjSNAxGq56Nfndb2e5R/PP2wjo/IzrL6/Mem+Mtr4/29z9hml3F3HXO5jpvPtY/I2X35Vh3fcIjJ96Wtvrbv9E8zdNiv35lWV71zzfM4smVLXHOsQmjzmFYZ5o/g4tvt5SyTG76l48I3Rui4aEpaS2trZB7rTrafZ/GW1/qPkfOKTH6WDs63xv37l2N1fHTUaoZVpea1OqPEPFcrO54lXRJd49nS87wfD/VZLyItYw1USg0VkaEiInWFtVYhQx7dQB7dQB7dQB7dQB7dQB7dQB4dkfSutp7nefK/5w77+8d7ntfT87yeWQFvBoPEkUc3kEc3kEc3kEc3kEc3kEc3kMeaLdFPPDcopQo8z1unlCoQkbRsz1ryC7vEde7lD/ha9jV1ljKlBl+c3VnHkZUrk55Hzpt2ScJRD5ldIedeZuZ0W8t51rgBjXxlJ8GX2lYkFHmMNuVYU26ZkeD/VvWZ/xsd73r9IB3v7rPTGlfwpHlR+u5E++lw11lmR9WB9bbr+NphE61xT3wyQMfe5wsTmm+SQpnHymwf1FvHL995j68ndr57vPhnHRf6SmsdUuPymEqbT2hV9SAR2VVmntP5y+xdbFNUdhT6PJbt3h2z3XHkd0nf/6SDzO7wk+rZO8yvG23yM/Pof+t4Z1mJNa54XHsdN5C0lH+nLY8qx/yMVlzczurzl9fO32uXv14waYSOY+1cWxn/Dp0iIq1v+ybGSJGc/5j4hiEX6fiOLXaBYfF55nfruWMetfr+8sLZZtybZkfdg8fYuxh7JfbvxgEK/fMxaOuH9dLx8p+Psfpe2NlUx13uMbsMh7BEs0bnMSMvT8fbBxxu9RX8ySzdm9jxqbju79kdBVZ77O1n6bjxogN/7qdSop94vi4iPy6eGSwiye3FjnQhj24gj24gj24gj24gj24gj24gj46I5ziViSLyiYgcopRaq5QaIiJ3iMgpSqllInJyeRvh1kHIowvIoxvIoxvIoxvIoxvIoxvIo8Pi2dV2UIyuftU8FwRrled5PSv4PnmsWcijG8ijG8ijG8ijG8ijG8ijwxJd4xkKxd3trcQr22K/8P0hJl61IOa46lD/a7Pm2T+nfdFLoceb7Y/rXHCw1VX6bfLrbGqTybsbWO0Gw82H+XlLfOsBx0pMHabY7Xs/+a2OB44dp+Oz8zdb49Y/ZbaLf7NrozhmW/vsO/loq33u38wW7i0yY6/rHLTqFB13fsasz425qwCcUaddG6vd5y/xHXX19ycu0HHrhU6uBQ690vXmCJXMQwqtvhe6TfC1zPrP4jL7Wd1gYnz5dtGKW8yRb1+dPybmuLM/ucRqdxyVnrVdTSfEftx2L5v4jXndrL63u7xqGmaJp/RdeZk1rt4rM5OZXq0T/Zx7+s/3+lrZVt8tT5jPllov4/UyKEtvNccEfnXOgwndR9GUS3Xc5W/2sUWNN4R7Xadf0rvaAgAAAABQGS48AQAAAACBqtGltied+XnVg8oV3Wq29I6Ulh7wY2UWdrDaylcWtO7n9rbG40beH9d9vlJkNuXqf7hdWpJdi0tt79vUV8f/PGhWzHF9h5mfWf159s7akeXLk55H/rKtOn7rB1MO2j/XPorgZ3lf6Xhqt4usvrK5i5OeR42llA7XX2pvhz+sUcXHGL26q5HV3nlxEx17i4MtkUe4fDXCPj7l5RYvxxhpa7rwwF/fEZzFIxtZ7bZ1ciseCO2nJ8b3WtfwvZr1s3x/+PFW+/TH5+i4a5YpAe0w0v67ueGVQKflnLW/bGG1/T/bc1eeavW1ude8j2YJS3DK8pI/oOaZfg/r+ALPLrM/5KHmOva+TMuxfnHjE08AAAAAQKC48AQAAAAABKpGl9oeiE13myKCpn89VMdl876yxilfScI3I81uzr89911r3D4vU8fXNXshoTl9V2rKDzNLYu/IW9tMnXysjv95cexS2/pLtuo4snxV0o9b8otjrPambmbX5M5Z/p1s7fImfxnLNwMaW32t5yY9rRors6iTjucf91TMcdvLzO7Oj559mtVXtrgWlyrXQt5xZtfLBwc+bvVl+P6f9M3d9XV81auDrXGdXqs5u/u5KrOpKZE/4bClcd2m/+tXWe3OUnt3Ms3wFT1mKvvzgYhn3iuo5Kv3UirzvS+s9vwSU05/ZLb5G/tUuw+tcT+X7oHOywXfXW3KmF8adrfVN7ukro433WEvG8spmR3sxCAiIl2uNcu/+k67POa4788xS7mOaW3vXPu3g9/U8dLTxll9M/qZ96s3DjOneORMCV9++cQTAAAAABAoLjwBAAAAAIHiwhMAAAAAEKgavcbz7ek97G/87uOYY2d0f07H459rr+N7Pu5vjauTv0/HC/rcp+OMqGv0Mkl+TeaZd47UcYv3/5v0/bki/2uzvmWnZx/Dka9ydLzmVrPONv/53gk9lnfhJh0/1XW01dfJ2va/Zm1bHwYrLmhe9SAR2eVbs5TK42c2XmFv7b+zjfm9+9Wpn1p90yccp+MWD/JcTYZ/LfWONvafoFbnm7XaJ+XutPr8r7iPfXeCjovutNcQ1rBlb04q62jW7k1o+0Rct8kpzqx6UC1RUHebjv1rOkVEnt5xkI6bPFaz1jNH+trv2XrkmNfSiFc3ejiqkFHfrHW/5RKzj0KnqCOLTpt0hY47/qdm/c64IrJli47zX4i9fj3ft2XMhqi+K/pcqeN1w/dafZOPNket3DzmER3f+bPTrXGlq7+JZ7qB4hNPAAAAAECguPAEAAAAAASqRpfadvirXTJw7IbhOj5tsF12e1vLeToe2nC1iaO2JLaZ6/KbirtbPc/OMqWdjb+0f4wz/zamwnvLUnYpUcG09TqmPMzI21Cq491l9k8m3/cjnHPsM6ZxrFSDxMppP99r5tj+uW+tvtLowfgfBZn1dDx6tV3G+sQWUw77/EyT5OZttljjMpQpky3Ksrfs95v1E1OOkv/TnJjjNpf9YLVnr+oZY2TtVqfAlP1FWjXT8bfX2+WB47qZ52rnrBk6bpiRbY3zL2mobDHDpMI3dHzx5H5W3+w3ze9Mh4eWWX2R4uJK7rX2qtOmtY6/Pr+tjk88K/ZzqTLNsxO7Hfa7tcV8HUe8SgbWMFsOsV9zi7IqLq/tNvNCq91KFgY2p5psyZjOOh5Y7wMdn7HMPpas863m58d7zZor44MvddzqA7vvzMvN0r3XRt2l46+GH2yNK/wzpbYAAAAAAMdx4QkAAAAACBQXngAAAACAQNXoNZ7RDhpt1ofNebal1XfUOZfrOKP/Jokl67kmOm4ya6Pp2LrDGldUPFvH/uMBRGIftVL4/hC7vWpBzHnUZjlTzM92QyTL6msRwh33t0bMGsXSVV+ncSbh0n6yOQ7jo9/aLzUn1K149Wv0mp/bWpi1YrcNTH7dmP84npd2Nbb67lzycx03Gl3f6st5Z7bUVhl1TU6W/LOb1fef0/6l48Is87Ot/Lip7Er6YrtybV8d7yo193FiY/s4lQmXvGfmd2FDq++vz/xOx21v4licHy262fy9XHrqA2mcCarSJ3eljh+86Cyrr8nj6T8qI7Ox/bq6t1sHHTf4zXdx3UfbP6y12qxL3G/9CPsIsGUnm/1ERq43+xDsvd5+/6u2zwl0Xki/FmPN37Npwwp1nHnQDxUNTys+8QQAAAAABIoLTwAAAABAoJwqtfWL3ja/xYO+9oNx3kecj7W9fXw/xqJbd9v3X8phG1U5Z9IIq33rmf/W8Vl5W6Q6Tdje2mqv2NNCx82yTKn1VY3tIxqmbT/c13Jo7/tkzTJHAtw14Eyra8hl5uiN3LbmZzu319OBTqnfQjOPvKF2OWiz1Uujh9dKO87rbbV/9/fJOr6o4UdRo7OkOi3ca14TBz35Z6uv3T8+07G3b5eOJxccYY174Pdn6PiuIY9ZfZ9e/E8dX9TvdB2XnGvX8JeuWy+1yWE3mmUlp7Qy5ZvHNItv6cAX37ex2u3zv9fxuDYfRA/Xdnt7dZxbzGtnPNrWMUs7Lh35itX33PJf6Djjoy8lSBn1zXKEjEampH1nd/v4hnfHVXZknfHOD6ZU34tUVqpfu2TUM/m++8pHYo5782Xzut1mBssIEF584gkAAAAACBQXngAAAACAQDlbahu0zAYNdNx/6IyY4xbvNSUjkUWU8h2oDqPsXfqeePJUHb/5uCmH61xvo8Tj7fVdrPbuZwt03Px9eye90q/X6FgdY3aTu+pVu9R26jPH6bhAKHGpSGTJcqtdOMLXVkqHA+v3Tej+1ww15c5z/jwm5rh1W8zztv3qeQk9louKLzW/w7ddbZen9sv1LxGI/X+VWcqUq+6Ls2qy34LfWO3cn6/Scduo51Ksu4wui219u2nff/uhVt/VfzfP43mXmN1b733bHvfuEXmxJ+0g/2tdjnmJlXifIdn17WUPW/6TG9ftluwzb0GaP5T+HVnD4shZg3T85THPxhz3+wb2LrEnPGNe+84ce42O205YYo2LbNp8wHPyl3yKiKy9xJS472pvFiZlbYv/84zCty/W8aH/MuXzZTu+OuD5uWrF381O4v1yP7b6Or9oTmsousu3FCH4aSFkVt5h/oafmW+WlNyZjslUgU88AQAAAACBqvLCUynVRin1nlJqkVJqoVJqePn3myilpimllpX/27iq+0JaZZFHJ5BHN5BHN5BHN5BHN5BHN5BHh8XziWepiPzF87zDRKS3iFyhlDpMREaJyDue53UWkXfK2wg38ugG8ugG8ugG8ugG8ugG8ugG8uioKtd4ep63TkTWlcc7lFKLRaSViJwhIn3Lhz0pIu+LyLWBzDKENpzXVcc3tLg/5jj/kQDRa5ZSbJ/neV+I1Ow8Rhab9ZXf+U59+E7iW1OUI6tjtmvI4TZO5NHimRUpke3bE7qLzL2V9KlQrigIVR796zpPyt1p9cV7sIF/XWdZ1K2+j5To+Pip5jWxyzX2uvd4j7BKVPu7vtBx1y5/0HGLxjuscXmyMt67DFUe06W0WyerPanThLhud94Hl+q4s3xerXM6QKHKY+vrzDPh1Pt/ZfW93eXVmLfrVMf8HZz7J7Pec8Lv7aPC7lt0ko7rvm3WvX/fa5817uKe5vikuhn2a/PwxrHX0sdSOHWo1T70ykU6Ltu9O3p4IkKVx0RlNm2i43vOelLHg1adYo079P/Mzy+yr5I/gjWPE3kM0vZB9rFns8436zrzM3Kih4fKAW0upJRqLyJHichMEWlZflEqIrJeRFrGuM1QERkqIlJX6lU0BClGHt1AHt1AHt1AHt1AHt1AHt1AHt0T90cBSql8EXlJREZ4nmf915fneZ7E2EjL87zxnuf19DyvZ5aE+yq8NiCPbiCPbiCPbiCPbiCPbiCPbiCPborrE0+lVJbsT/6znue9XP7tDUqpAs/z1imlCkQkvvMsHHHzyMd1nFFDNgcmj4lTe00h7rcRuySopKl57cts1tTqS2Tb+irnQh4PSMQzZZ/9OprSzpVZ2dY4L8WlSmHK42UfXqjjN372gNVXmBVfYcysEnMszuBPL7b6mk+uq+OiiZ/qOOjS2mhle/bouNNF5ogJlWuX6h/IvMKUx5omd0l43hSGKY/+o9eyfmG/Tp3Sxzy3Lh37otV3Vp59rM2PhjSwjwob0vtp0+gtMfmXKfhfR6PN32tKdM+b/Uerr/lE84nToVPnW33VVF5rCVMeE7X4ng46Pq3edB3fMqHIGtdku7tHELmQx2RlHlJotRdf00jHr5082urzl9eO39Zex4U328+xVP/NrUg8u9oqEZkgIos9z7vX1/W6iAwujweLyGvVPz1UM/LoBvLoBvLoBvLoBvLoBvLoBvLoqHj+K/snInKhiMxXSs0p/951InKHiDyvlBoiIl+LyDmBzBDVJV/IowvIoxvIoxvIoxvIoxvIoxvIo8Pi2dX2YxFRMbr7Ve90EKCdnueRx5qPPLqBPLqBPLqBPLqBPLqBPDrsgHa1hRHxTJVy9NEBi/eadqsP9ghqvrK5i3U88At7/dqi3z+o4yN3Xmn1tb49rUfoIEpuhm8dZ0ZW+iYSMkV/+EzH1xRdYPVt7NMirvto+ohZb9RJ5lTLvILkX+8pe3idTsbeRtlVDypX4pn1gFm7gpiNW6LXnmdNN8fOPHFCL6tv3BFtzO2u3aTj6Ye9ktBjd3x7iLm/qG1cDn7TvH42mvGNjtt9a6/j9Iv3aKba7pzuJsdf7TNHUTWfssIaF4b1ejCi9/hoNtm81l110DQdnzXjUmtc9nKzx8Ce1ub5Pv2U0da4tnX8exHY718mbGur4//82rwuRJYsk7CpGbviAAAAAABqLC48AQAAAACBotQ2AF2yzfX8t33MMQJt30vHbJBKkdwKj5VCwA5+a4OO111tbx9ekGm2839j+eE6bl8yL/iJ1UCRpXY5V9OoNiAiktm5o47Pv/uNuG/38Z6GOm55P0sRkhHZYJ8mkeVrqw9M+fPAvJMSuv/OWz+vepCIlFY9BAm6Z/2pOo7ON8JF1bEvqY6ov1rHXbNN31cnPWrfMObTMzdWh3R9yl7W1elW836mbNfySueZbnziCQAAAAAIFBeeAAAAAIBAUWobgD2eKTypszONE0HK5R21Od1TqJX85aE3fNff6hvf5kMdZyzKT9mcAJdFlq3U8cS1x1h9Fx22JubtNkd4DqaCfzfcyNa9lYxEmBXv8T9fdqRtHqha6foNVnvSaFMmnTnCLMMa1jj2TrOv72qs45FTfmv1HfLIFh13WPSp1VcWve10iPGJJwAAAAAgUFx4AgAAAAACxYUnAAAAACBQrPEMQPdpZpvjonvYLr42GX/4M1b7+jrH69grZdP5VFjb215YPUB66Lit8HwEUqlMyqz2XQ+dq+ODeD4C/2POUf7WunRNA0lq+ugnOp76aAMTy9Fx3b5Q7HWckeqZVtrxiScAAAAAIFBceAIAAAAAAkWpbYIe7Fxk4qi+Ivk8tZNBSnkfNra/4TtJoGWmvW393p9113HW258FOCsASI/tLx5stUcPM38fx887werrOJryWgCorfjEEwAAAAAQKC48AQAAAACB4sITAAAAABAo1ngCB6jgXnuN0oB7e8QYKZIlrOsE4LZmD39itac/XF/HHWVOimcDAAgrPvEEAAAAAASKC08AAAAAQKCU53mpezClikXkaxFpJiKbUvbAFQvDHERSN492nuc1r447Ks/jLqldP7+qkMfkkMcEkccKkcfkkMcEkccKkcfkkMcEkccKpTWPKb3w1A+q1Gee5/VM+QOHbA5hmseBCsu8mUdywjJv5pGcsMybeSQnLPNmHskJy7yZR3LCMm/mkZywzJt57EepLQAAAAAgUFx4AgAAAAACla4Lz/Fpely/MMxBJDzzOFBhmTfzSE5Y5s08khOWeTOP5IRl3swjOWGZN/NITljmzTySE5Z5Mw9J0xpPAAAAAEDtQaktAAAAACBQKb3wVEr1V0otUUotV0qNSuHjPqaU2qiUWuD7XhOl1DSl1LLyfxunYB5tlFLvKaUWKaUWKqWGp2suySCP5DHJxyWP1Yg8ksckH5c8ViPySB6TfFzyWI3IYwjz6HleSr5EJFNEVohIRxHJFpG5InJYih77RBHpISILfN+7S0RGlcejROTOFMyjQER6lMf1RWSpiByWjrmQR/JIHskjeQzHF3kkj+QxPF/kkTySxwDnlMJfgONEZKqv/VcR+WsKH7991C/AEhEp8CVmSarm4pvDayJyShjmQh7JI3kkj+SRPJJH8kgeySN5JI9BfaWy1LaViKzxtdeWfy9dWnqet648Xi8iLVP54Eqp9iJylIjMTPdcDhB59CGP1YY8JoY8+pDHakMeE0MefchjtSGPiSGPPmHJI5sLiYi3/5I/Zdv7KqXyReQlERnhed72dM7FJeTRDeTRDeTRDeTRDeTRDeTRDbU5j6m88PxWRNr42q3Lv5cuG5RSBSIi5f9uTMWDKqWyZH/yn/U87+V0ziVB5FHIYwDIY2LIo5DHAJDHxJBHIY8BII+JIY8Svjym8sJztoh0Vkp1UEpli8h5IvJ6Ch8/2usiMrg8Hiz7654DpZRSIjJBRBZ7nndvOueSBPJIHoNAHhNDHsljEMhjYsgjeQwCeUwMeQxjHlO8qHWA7N9RaYWIXJ/Cx50oIutEZJ/sr/EeIiJNReQdEVkmItNFpEkK5vFT2f9x9jwRmVP+NSAdcyGP5JE8kkfyGJ4v8kgeyWN4vsgjeSSPwXyp8okBAAAAABAINhcCAAAAAASKC08AAAAAQKC48AQAAAAABIoLTwAAAABAoLjwBAAAAAAEigtPAAAAAECguPAEAAAAAASKC08AAAAAQKD+HwnQ5U80Jw9SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1,8, figsize=(16,4))\n",
    "for x, ax in zip(inputs, axes):\n",
    "    ax.imshow(x.squeeze(0))\n",
    "    \n",
    "#plot out the first 8 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea3aa7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "#load back saved model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e32f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(inputs)\n",
    "#subs testing inputs into saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a52e7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:      9     9     4     2     7     0     1     3\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(8)))\n",
    "\n",
    "#test the network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "858e0099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "#initialize number of testset\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        #max function will give 2 outcome\n",
    "        #x_random = torch.randn(8,10)\n",
    "        #torch.max(x_random,1) \n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy: %d %%' % (100 * correct / total))\n",
    "\n",
    "#calculate the accuracy of the network on the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31850b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8.10",
   "language": "python",
   "name": "python3.8.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
